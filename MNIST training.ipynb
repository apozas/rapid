{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient training of energy-based models via spin-glass control [(arXiv:1910.01592)](https://arxiv.org/abs/1910.01592)\n",
    "---\n",
    "\n",
    "## Training on MNIST\n",
    "\n",
    "**Authors**: Gorka Muñoz-Gil and Alejandro Pozas-Kerstjens\n",
    "\n",
    "In this notebook we show how to train a RA-PID model on the MNIST dataset for generative purposes. For all the details on its theory, please take a look at [the associated paper](https://arxiv.org/abs/1910.01592). The example we show here corresponds to the models used to generate Fig. 6 from that paper.\n",
    "\n",
    "Let's get into work. First, you will need to get two repositories: [ebm](https://github.com/apozas/ebm-torch), from which our models are created and [rapid](https://github.com/apozas/rapid), which contains all the details on training with RA and PID. This notebook should be placed on the same folder both repositories have been cloned, as:\n",
    "```\n",
    "your_folder\n",
    "│   rapid\n",
    "│   ebm\n",
    "│   MNIST training.ipynb\n",
    "```\n",
    "\n",
    "We will then load the following objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapid.rapid import RA_RBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For complex datasets, as shown in **Section V.4.B** of the paper, RAPID can be enhanced by performing a series of Gibbs Steps (GS) to the auxiliary patterns. Moreover, we will consider continuos patterns, restricted to the range $[-1,1]$. We must update our imported RA model in order to do such operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "class RA_RBM_GS(RA_RBM):\n",
    "    ''' RAPID Boltzmann machine with Gibbs Steps performed to the patterns for\n",
    "    the calculation of the negative phase'''\n",
    "    def __init__(self, n_visible=100, n_hidden=50, K=50, gibbs_steps = 0, \n",
    "                 optimizer=None, device=None, xi=None, sampler = None):\n",
    "        super().__init__(n_visible, n_hidden, K, optimizer, device, xi) \n",
    "        \n",
    "        self.gibbs_steps = gibbs_steps\n",
    "        self.sampler = sampler\n",
    "        \n",
    "    def gibbs_steps_patterns(self):\n",
    "        ''' Function that perfoms the Gibbs Steps to the patterns'''\n",
    "        zero = copy.deepcopy(self.xi)\n",
    "        for i in range(self.gibbs_steps):            \n",
    "            zero[:, self.n_visible:] = self.sampler.get_h_from_v(\n",
    "                                                       zero[:, :self.n_visible],\n",
    "                                                       self.weights,\n",
    "                                                       self.hbias)\n",
    "            zero[:, :self.n_visible] = self.sampler.get_v_from_h(\n",
    "                                                       zero[:, self.n_visible:],\n",
    "                                                       self.weights,\n",
    "                                                       self.vbias)  \n",
    "        return zero[:, :self.n_visible]     \n",
    "        \n",
    "    def train(self, input_data):\n",
    "        ''' Adapt usual training to get the negative phase by the\n",
    "        function gibbs_steps_patterns'''\n",
    "        \n",
    "        for b, batch in enumerate(input_data):\n",
    "            \n",
    "            # Get data, positive and negative phase\n",
    "            sample_data = batch.float()\n",
    "            vpos = sample_data            \n",
    "            vneg = self.gibbs_steps_patterns()\n",
    "            \n",
    "            # Weight updates. Includes momentum and weight decay if necessary\n",
    "            xi_update = self.optimizer.get_updates(vpos, vneg, self.xi)            \n",
    "            self.xi += xi_update            \n",
    "            \n",
    "            # Restrict the patterns to the range +- 1\n",
    "            self.xi[self.xi > 1] = 1\n",
    "            self.xi[self.xi < -1] = -1\n",
    "                    \n",
    "            # Generate new weights from the patterns\n",
    "            self.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the rest of necessary functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapid.rapid import SGD_xi, ContrastiveDivergence_pm as CD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need some general modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import Lambda\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the MNIST dataset from `torchvision`. We will normalize it and create a validation dataset. To start, you should choose in which device (CPU or GPU) you want the model to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose device\n",
    "gpu    = True     \n",
    "device = torch.device('cuda' if gpu and torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the data from torchvision\n",
    "threshold  = lambda x: (x > 0.3)\n",
    "normalize  = lambda x: x / 255\n",
    "data_mnist = datasets.MNIST('mnist', train=True, download=True)\n",
    "data = data_mnist.train_data.type(torch.float)\n",
    "\n",
    "# Normalize the data\n",
    "data = torch.FloatTensor(data)\n",
    "data = Lambda(normalize)(data)\n",
    "data = Lambda(threshold)(data).type(torch.float)\n",
    "data_transformed = (2 * (data.view((-1, 28**2))) - 1).to(device)   \n",
    "\n",
    "# Create a validation set\n",
    "ratio_val = 0.95\n",
    "data = data_transformed[:int(len(data_transformed)*ratio_val)]\n",
    "val_set = data_transformed[int(len(data_transformed)*ratio_val):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set our machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidd          = 3000     # Number of nodes in the hidden layer\n",
    "vis           = 28**2    # Number of nodes in the visible layer\n",
    "K_hebb        = 200      # Number of auxiliary patterns\n",
    "learning_rate = 0.015    # Learning rate\n",
    "epochs        = 500      # Number of training epochs\n",
    "batch_size    = 8000     # Batch size\n",
    "gibbs_steps   = 1        # Number of Gibbs Steps to the auxiliary patterns\n",
    "\n",
    "rbm = RA_RBM_GS(n_visible=vis,\n",
    "                n_hidden=hidd,\n",
    "                sampler=CD(k=1),    # k is not used in this example\n",
    "                optimizer=SGD_xi(learning_rate),\n",
    "                device=device,\n",
    "                K=K_hebb,\n",
    "                gibbs_steps=gibbs_steps \n",
    "                ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:34<00:00,  3.25it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loader = torch.utils.data.DataLoader(data,\n",
    "                                               batch_size=int(batch_size),\n",
    "                                               shuffle=True)\n",
    "    rbm.train(train_loader)\n",
    "    \n",
    "    if epoch > 100 and rbm.optimizer.learning_rate > 1e-3:\n",
    "            rbm.optimizer.learning_rate *= 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we generate new images by Gibbs sampling, recycling the model's sampler (`CD` in this particular example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 25 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(5, 5, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    zero = torch.randint(high=2, size=(1, 28**2))\n",
    "    zero = (2 * zero - 1).type(torch.FloatTensor).to(device)   \n",
    "    for i in range(200):\n",
    "        zero = rbm.sampler.get_h_from_v(zero, rbm.weights, rbm.hbias)\n",
    "        zero = rbm.sampler.get_v_from_h(zero, rbm.weights, rbm.vbias)\n",
    "    ax.imshow(zero.detach().cpu().numpy().reshape(28,28), cmap='gray')\n",
    "    ax.set_axis_off()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
